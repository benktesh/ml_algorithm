{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bf4a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: click in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.10.23-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 22.0 MB/s  0:00:00\n",
      "Downloading regex-2025.10.23-cp313-cp313-win_amd64.whl (276 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, nltk\n",
      "\n",
      "   ---------------------------------------- 0/3 [tqdm]\n",
      "   ---------------------------------------- 0/3 [tqdm]\n",
      "   ---------------------------------------- 0/3 [tqdm]\n",
      "   ---------------------------------------- 0/3 [tqdm]\n",
      "   ---------------------------------------- 0/3 [tqdm]\n",
      "   ---------------------------------------- 0/3 [tqdm]\n",
      "   ------------- -------------------------- 1/3 [regex]\n",
      "   ------------- -------------------------- 1/3 [regex]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   ---------------------------------------- 3/3 [nltk]\n",
      "\n",
      "Successfully installed nltk-3.9.2 regex-2025.10.23 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917620eb",
   "metadata": {},
   "source": [
    "# ðŸ”¤ **Natural Language Processing (NLP) for Query Understanding**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ **Project Overview**\n",
    "This notebook demonstrates **NLP techniques** for understanding and processing user queries, particularly useful for:\n",
    "- **ðŸ” Search query analysis**\n",
    "- **ðŸ’¬ Chatbot preprocessing** \n",
    "- **ðŸ“Š Text classification**\n",
    "- **ðŸ› ï¸ IT support automation**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  **What You'll Learn**\n",
    "| **NLP Technique** | **Purpose** | **Real-World Application** |\n",
    "|-------------------|-------------|---------------------------|\n",
    "| ðŸ”¤ **Tokenization** | Break text into words/tokens | Query parsing for search engines |\n",
    "| ðŸ—‘ï¸ **Stop Word Removal** | Filter common words | Focus on meaningful content |\n",
    "| ðŸ·ï¸ **POS Tagging** | Identify word types | Extract key entities |\n",
    "| ðŸŒ¿ **Stemming/Lemmatization** | Normalize word forms | Improve search accuracy |\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ **NLTK Data Requirements**\n",
    "**Important**: NLTK requires specific data packages to be downloaded. We'll install:\n",
    "- `punkt_tab` - Modern sentence/word tokenizer\n",
    "- `stopwords` - Common words to filter out\n",
    "- `wordnet` - Lexical database for lemmatization\n",
    "- `averaged_perceptron_tagger` - Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c54b9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\BenkteshSharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "329d6fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'laptop', 'is', 'overheating', 'after', 'the', 'latest', 'update', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample query\n",
    "query = \"My laptop is overheating after the latest update.\"\n",
    "\n",
    "# Tokenize the query\n",
    "tokens = word_tokenize(query)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eafe2e",
   "metadata": {},
   "source": [
    "### ðŸŽ‰ **Tokenization Results Analysis**\n",
    "\n",
    "**Input Query**: `\"My laptop is overheating after the latest update.\"`\n",
    "\n",
    "**Tokenized Output**: `['My', 'laptop', 'is', 'overheating', 'after', 'the', 'latest', 'update', '.']`\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ” **Token Analysis**\n",
    "| **Token** | **Type** | **Importance** | **NLP Use** |\n",
    "|-----------|----------|----------------|-------------|\n",
    "| `My` | Pronoun | Low | Could be removed (stop word) |\n",
    "| `laptop` | Noun | **HIGH** | ðŸ–¥ï¸ Device identification |\n",
    "| `is` | Verb | Low | Could be removed (stop word) |\n",
    "| `overheating` | Verb | **HIGH** | ðŸ”¥ Problem identification |\n",
    "| `after` | Preposition | Medium | â° Temporal relationship |\n",
    "| `the` | Article | Low | Could be removed (stop word) |\n",
    "| `latest` | Adjective | Medium | ðŸ†• Specificity indicator |\n",
    "| `update` | Noun | **HIGH** | ðŸ”„ Cause identification |\n",
    "| `.` | Punctuation | Low | Sentence boundary |\n",
    "\n",
    "---\n",
    "\n",
    "> **ðŸ’¡ Next Steps**: Filter stop words, identify key entities, and classify the query intent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f547f354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Data Paths:\n",
      "  C:\\Users\\BenkteshSharma/nltk_data\n",
      "  c:\\Repo\\Scratch\\ml\\mlvenv\\nltk_data\n",
      "  c:\\Repo\\Scratch\\ml\\mlvenv\\share\\nltk_data\n",
      "  c:\\Repo\\Scratch\\ml\\mlvenv\\lib\\nltk_data\n",
      "  C:\\Users\\BenkteshSharma\\AppData\\Roaming\\nltk_data\n",
      "  C:\\nltk_data\n",
      "  D:\\nltk_data\n",
      "  E:\\nltk_data\n",
      "\n",
      "âœ… Punkt tokenizer found at: C:\\Users\\BenkteshSharma\\AppData\\Roaming\\nltk_data\\tokenizers\\punkt\n",
      "Contents: ['.DS_Store', 'czech.pickle', 'danish.pickle', 'dutch.pickle', 'english.pickle', 'estonian.pickle', 'finnish.pickle', 'french.pickle', 'german.pickle', 'greek.pickle', 'italian.pickle', 'malayalam.pickle', 'norwegian.pickle', 'polish.pickle', 'portuguese.pickle', 'PY3', 'README', 'russian.pickle', 'slovene.pickle', 'spanish.pickle', 'swedish.pickle', 'turkish.pickle']\n",
      "\n",
      "Forcing punkt download...\n",
      "âœ… Download completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\BenkteshSharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Check NLTK data path and verify punkt tokenizer\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "print(\"NLTK Data Paths:\")\n",
    "for path in nltk.data.path:\n",
    "    print(f\"  {path}\")\n",
    "\n",
    "# Check if punkt tokenizer exists\n",
    "punkt_path = None\n",
    "for path in nltk.data.path:\n",
    "    potential_punkt = os.path.join(path, 'tokenizers', 'punkt')\n",
    "    if os.path.exists(potential_punkt):\n",
    "        punkt_path = potential_punkt\n",
    "        break\n",
    "\n",
    "if punkt_path:\n",
    "    print(f\"\\nâœ… Punkt tokenizer found at: {punkt_path}\")\n",
    "    print(f\"Contents: {os.listdir(punkt_path)}\")\n",
    "else:\n",
    "    print(\"\\nâŒ Punkt tokenizer not found in any NLTK data path!\")\n",
    "    \n",
    "# Try to force download to specific location\n",
    "print(\"\\nForcing punkt download...\")\n",
    "try:\n",
    "    nltk.download('punkt', download_dir=r'C:\\Users\\BenkteshSharma\\AppData\\Roaming\\nltk_data')\n",
    "    print(\"âœ… Download completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Download failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a161fa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading punkt_tab tokenizer (newer version)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\BenkteshSharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\BenkteshSharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… punkt_tab downloaded successfully!\n",
      "\n",
      "Downloading additional NLP resources...\n",
      "âœ… stopwords downloaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\BenkteshSharma\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… wordnet downloaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\BenkteshSharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… averaged_perceptron_tagger downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Download the newer punkt_tab tokenizer\n",
    "import nltk\n",
    "\n",
    "print(\"Downloading punkt_tab tokenizer (newer version)...\")\n",
    "try:\n",
    "    nltk.download('punkt_tab')\n",
    "    print(\"âœ… punkt_tab downloaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Download failed: {e}\")\n",
    "\n",
    "# Also download other useful NLP resources\n",
    "print(\"\\nDownloading additional NLP resources...\")\n",
    "resources_to_download = ['stopwords', 'wordnet', 'averaged_perceptron_tagger']\n",
    "for resource in resources_to_download:\n",
    "    try:\n",
    "        nltk.download(resource)\n",
    "        print(f\"âœ… {resource} downloaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {resource} download failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38474f31",
   "metadata": {},
   "source": [
    "## ðŸ”¤ **Step 1: Text Tokenization**\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ **What is Tokenization?**\n",
    "**Tokenization** is the process of breaking down text into individual units (tokens) - typically words, but can also include:\n",
    "- **Words**: `\"Hello world\"` â†’ `[\"Hello\", \"world\"]`\n",
    "- **Sentences**: `\"Hi there. How are you?\"` â†’ `[\"Hi there.\", \"How are you?\"]`\n",
    "- **Subwords**: For handling unknown words in deep learning\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ› ï¸ **Use Case: IT Support Query Processing**\n",
    "Our example query: *\"My laptop is overheating after the latest update.\"*\n",
    "\n",
    "**Why tokenize this?**\n",
    "- **ðŸ” Keyword extraction**: `laptop`, `overheating`, `update`\n",
    "- **ðŸ·ï¸ Problem classification**: Hardware vs. Software issue\n",
    "- **ðŸ“Š Intent recognition**: Technical support request\n",
    "- **ðŸ”— Entity linking**: Connect to knowledge base articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17e9fb83",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character 'ðŸ”' (U+1F50D) (2462048576.py, line 17)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m| ðŸ” **Entity Extraction** | Find all nouns = potential entities | `laptop`, `update` |\u001b[39m\n      ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character 'ðŸ”' (U+1F50D)\n"
     ]
    }
   ],
   "source": [
    "## ðŸ·ï¸ **Step 2: Part-of-Speech (POS) Tagging**\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ **What is POS Tagging?**\n",
    "**Part-of-Speech Tagging** identifies the grammatical role of each word in a sentence:\n",
    "- **Nouns** (NN): People, places, things\n",
    "- **Verbs** (VB): Actions, states  \n",
    "- **Adjectives** (JJ): Descriptive words\n",
    "- **Prepositions** (IN): Relationship words\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ› ï¸ **Why POS Tagging Matters**\n",
    "| **Use Case** | **Example** | **Benefit** |\n",
    "|--------------|-------------|-------------|\n",
    "| ðŸ” **Entity Extraction** | Find all nouns = potential entities | `laptop`, `update` |\n",
    "| ðŸ“Š **Sentiment Analysis** | Adjectives carry emotion | `latest` (neutral) |\n",
    "| ðŸ¤– **Chatbot Intent** | Verbs indicate actions | `overheating` (problem) |\n",
    "| ðŸ“ **Text Summarization** | Focus on key nouns/verbs | Core concepts |\n",
    "\n",
    "---\n",
    "\n",
    "### âš ï¸ **Modern NLTK Requirements**\n",
    "**Important**: Newer NLTK versions require language-specific taggers:\n",
    "- âœ… Use: `averaged_perceptron_tagger_eng` (English-specific)\n",
    "- âŒ Old: `averaged_perceptron_tagger` (generic, deprecated)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a684b98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words and their POS Tags:\n",
      "Natural: JJ\n",
      "language: NN\n",
      "processing: NN\n",
      "enables: VBZ\n",
      "computers: NNS\n",
      "to: TO\n",
      "understand: VB\n",
      "human: JJ\n",
      "language: NN\n",
      ".: .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural language processing enables computers to understand human language.\"\n",
    "\n",
    "# Tokenize text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Apply POS tagging\n",
    "pos_tags = pos_tag(words)\n",
    "\n",
    "# Print words with their POS tags\n",
    "print(\"Words and their POS Tags:\")\n",
    "for word, tag in pos_tags:\n",
    "    print(f\"{word}: {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ddce7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading language-specific POS tagger...\n",
      "âœ… averaged_perceptron_tagger_eng downloaded successfully!\n",
      "\n",
      "Downloading additional tagger resources...\n",
      "âœ… universal_tagset downloaded successfully!\n",
      "\n",
      "ðŸ” Checking tagger data location...\n",
      "Taggers found at: C:\\Users\\BenkteshSharma\\AppData\\Roaming\\nltk_data\\taggers\n",
      "âœ… English POS tagger is properly installed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\BenkteshSharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\BenkteshSharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the correct POS tagger for modern NLTK\n",
    "import nltk\n",
    "\n",
    "print(\"Downloading language-specific POS tagger...\")\n",
    "try:\n",
    "    # Download the English-specific perceptron tagger\n",
    "    nltk.download('averaged_perceptron_tagger_eng')\n",
    "    print(\"âœ… averaged_perceptron_tagger_eng downloaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Download failed: {e}\")\n",
    "\n",
    "# Also download other language versions if needed\n",
    "print(\"\\nDownloading additional tagger resources...\")\n",
    "additional_taggers = ['universal_tagset']\n",
    "for tagger in additional_taggers:\n",
    "    try:\n",
    "        nltk.download(tagger)\n",
    "        print(f\"âœ… {tagger} downloaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {tagger} download failed: {e}\")\n",
    "\n",
    "print(\"\\nðŸ” Checking tagger data location...\")\n",
    "import os\n",
    "for path in nltk.data.path:\n",
    "    tagger_path = os.path.join(path, 'taggers')\n",
    "    if os.path.exists(tagger_path):\n",
    "        print(f\"Taggers found at: {tagger_path}\")\n",
    "        if os.path.exists(os.path.join(tagger_path, 'averaged_perceptron_tagger_eng')):\n",
    "            print(\"âœ… English POS tagger is properly installed!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ead59ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words and their POS Tags:\n",
      "Natural: JJ\n",
      "language: NN\n",
      "processing: NN\n",
      "enables: VBZ\n",
      "computers: NNS\n",
      "to: TO\n",
      "understand: VB\n",
      "human: JJ\n",
      "language: NN\n",
      ".: .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural language processing enables computers to understand human language.\"\n",
    "\n",
    "# Tokenize text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Apply POS tagging\n",
    "pos_tags = pos_tag(words)\n",
    "\n",
    "# Print words with their POS tags\n",
    "print(\"Words and their POS Tags:\")\n",
    "for word, tag in pos_tags:\n",
    "    print(f\"{word}: {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4734d10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'PRP$'), ('laptop', 'NN'), ('is', 'VBZ'), ('overheating', 'VBG'), ('after', 'IN'), ('the', 'DT'), ('latest', 'JJS'), ('update', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Apply POS tagging to the tokens\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0d9dc6",
   "metadata": {},
   "source": [
    "### ðŸŽ‰ **POS Tagging Results Analysis**\n",
    "\n",
    "**Input Text**: `\"Natural language processing enables computers to understand human language.\"`\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ” **Detailed POS Tag Breakdown**\n",
    "\n",
    "| **Word** | **POS Tag** | **Full Name** | **Function** | **Importance** |\n",
    "|----------|-------------|---------------|--------------|----------------|\n",
    "| `Natural` | **JJ** | Adjective | Describes \"language\" | ðŸ” Qualifier |\n",
    "| `language` | **NN** | Noun (singular) | Subject/Object | ðŸŽ¯ **KEY ENTITY** |\n",
    "| `processing` | **NN** | Noun (gerund) | Action as concept | ðŸŽ¯ **KEY ENTITY** |\n",
    "| `enables` | **VBZ** | Verb (3rd person) | Main action | âš¡ **CORE ACTION** |\n",
    "| `computers` | **NNS** | Noun (plural) | Actor/Agent | ðŸŽ¯ **KEY ENTITY** |\n",
    "| `to` | **TO** | Infinitive marker | Grammar connector | ðŸ”— Structural |\n",
    "| `understand` | **VB** | Verb (base form) | Goal action | âš¡ **PURPOSE** |\n",
    "| `human` | **JJ** | Adjective | Describes \"language\" | ðŸ” Qualifier |\n",
    "| `language` | **NN** | Noun (singular) | Object | ðŸŽ¯ **KEY ENTITY** |\n",
    "| `.` | **.** | Punctuation | Sentence boundary | ðŸ“ Structure |\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸŽ¯ **Extracted Insights**\n",
    "- **ðŸŽ¯ Key Entities**: `language`, `processing`, `computers` (all nouns)\n",
    "- **âš¡ Main Actions**: `enables`, `understand` (verbs)\n",
    "- **ðŸ” Qualifiers**: `Natural`, `human` (adjectives)\n",
    "- **ðŸ“ Topic**: Computer-human language interaction\n",
    "\n",
    "---\n",
    "\n",
    "> **ðŸ’¡ Next Step**: Use these POS tags to extract meaningful entities and build smarter NLP applications!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20976ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸ IT Support Query Analysis\n",
      "==================================================\n",
      "Query: My laptop is overheating after the latest update.\n",
      "\n",
      "ðŸ·ï¸ POS Tagging Results:\n",
      "My           â†’ PRP$   (Possessive Pronoun)\n",
      "laptop       â†’ NN     (Noun (singular))\n",
      "is           â†’ VBZ    (Verb (3rd person singular))\n",
      "overheating  â†’ VBG    (Verb (gerund/present participle))\n",
      "after        â†’ IN     (Preposition)\n",
      "the          â†’ DT     (Determiner)\n",
      "latest       â†’ JJS    (Adjective (superlative))\n",
      "update       â†’ NN     (Noun (singular))\n",
      ".            â†’ .      (Punctuation)\n",
      "\n",
      "ðŸ“Š Extracted Information:\n",
      "ðŸŽ¯ Key Entities (Nouns): ['laptop', 'update']\n",
      "âš¡ Actions (Verbs): ['is', 'overheating']\n",
      "ðŸ” Qualifiers (Adjectives): ['latest']\n",
      "\n",
      "ðŸ¤– AI Analysis:\n",
      "ðŸ“± Device Type: Laptop/Hardware\n",
      "ðŸ”¥ Problem Type: Thermal/Performance\n",
      "ðŸ”„ Trigger Event: Software Update\n"
     ]
    }
   ],
   "source": [
    "# Apply POS tagging to our original IT support query\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Helper function to explain POS tags\n",
    "def get_tag_description(tag):\n",
    "    tag_descriptions = {\n",
    "        'PRP$': 'Possessive Pronoun',\n",
    "        'NN': 'Noun (singular)',\n",
    "        'NNS': 'Noun (plural)',\n",
    "        'VBZ': 'Verb (3rd person singular)',\n",
    "        'VBG': 'Verb (gerund/present participle)',\n",
    "        'IN': 'Preposition',\n",
    "        'DT': 'Determiner',\n",
    "        'JJS': 'Adjective (superlative)',\n",
    "        'JJ': 'Adjective',\n",
    "        '.': 'Punctuation'\n",
    "    }\n",
    "    return tag_descriptions.get(tag, 'Other')\n",
    "\n",
    "# Original IT support query\n",
    "it_query = \"My laptop is overheating after the latest update.\"\n",
    "\n",
    "print(\"ðŸ› ï¸ IT Support Query Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Query: {it_query}\")\n",
    "print(\"\\nðŸ·ï¸ POS Tagging Results:\")\n",
    "\n",
    "# Tokenize and tag\n",
    "tokens = word_tokenize(it_query)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Enhanced analysis with categories\n",
    "nouns = []\n",
    "verbs = []\n",
    "adjectives = []\n",
    "other = []\n",
    "\n",
    "for word, tag in pos_tags:\n",
    "    print(f\"{word:12} â†’ {tag:6} ({get_tag_description(tag)})\")\n",
    "    \n",
    "    # Categorize words\n",
    "    if tag.startswith('NN'):  # Nouns\n",
    "        nouns.append(word)\n",
    "    elif tag.startswith('VB'):  # Verbs\n",
    "        verbs.append(word)\n",
    "    elif tag.startswith('JJ'):  # Adjectives\n",
    "        adjectives.append(word)\n",
    "    else:\n",
    "        other.append(word)\n",
    "\n",
    "print(f\"\\nðŸ“Š Extracted Information:\")\n",
    "print(f\"ðŸŽ¯ Key Entities (Nouns): {nouns}\")\n",
    "print(f\"âš¡ Actions (Verbs): {verbs}\")\n",
    "print(f\"ðŸ” Qualifiers (Adjectives): {adjectives}\")\n",
    "\n",
    "# IT Support Classification\n",
    "print(f\"\\nðŸ¤– AI Analysis:\")\n",
    "if 'laptop' in [w.lower() for w in nouns]:\n",
    "    print(\"ðŸ“± Device Type: Laptop/Hardware\")\n",
    "if 'overheating' in [w.lower() for w in tokens]:\n",
    "    print(\"ðŸ”¥ Problem Type: Thermal/Performance\")\n",
    "if 'update' in [w.lower() for w in nouns]:\n",
    "    print(\"ðŸ”„ Trigger Event: Software Update\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75388ede",
   "metadata": {},
   "source": [
    "## ðŸ”§ **NLTK Data Issue Resolution Summary**\n",
    "\n",
    "---\n",
    "\n",
    "### âŒ **The Problem You Encountered**\n",
    "You experienced missing NLTK data at `C:\\Users\\BenkteshSharma\\AppData\\Roaming\\nltk_data\\tokenizers` because:\n",
    "\n",
    "1. **Version Incompatibility**: Newer NLTK versions require language-specific data packages\n",
    "2. **Outdated Downloads**: Old generic packages (`punkt`, `averaged_perceptron_tagger`) were deprecated\n",
    "3. **Path Confusion**: Data was partially there but in wrong format\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **The Complete Solution**\n",
    "\n",
    "| **Old Package** | **New Required Package** | **Status** |\n",
    "|-----------------|--------------------------|------------|\n",
    "| `punkt` | `punkt_tab` | âœ… **Fixed** |\n",
    "| `averaged_perceptron_tagger` | `averaged_perceptron_tagger_eng` | âœ… **Fixed** |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“ **Current Data Locations**\n",
    "Your NLTK data is now properly installed at:\n",
    "```\n",
    "C:\\Users\\BenkteshSharma\\AppData\\Roaming\\nltk_data\\\n",
    "â”œâ”€â”€ tokenizers/\n",
    "â”‚   â””â”€â”€ punkt_tab/           # âœ… Modern tokenizer\n",
    "â”œâ”€â”€ taggers/\n",
    "â”‚   â””â”€â”€ averaged_perceptron_tagger_eng/  # âœ… English POS tagger\n",
    "â”œâ”€â”€ corpora/\n",
    "â”‚   â”œâ”€â”€ stopwords/           # âœ… Stop words\n",
    "â”‚   â””â”€â”€ wordnet/             # âœ… Lexical database\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ **Key Takeaways**\n",
    "- Always use **language-specific** NLTK packages (`_eng`, `_tab` suffixes)\n",
    "- Test your NLP functions after downloading to verify they work\n",
    "- Modern NLTK is more precise but requires specific data packages\n",
    "- Your NLP pipeline is now fully functional for production use!\n",
    "\n",
    "---\n",
    "\n",
    "> **ðŸš€ Success**: Your NLP toolkit is now ready for advanced text processing, entity extraction, and intelligent query understanding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "285ee23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp313-cp313-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp313-cp313-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp313-cp313-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.10-cp313-cp313-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp313-cp313-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from spacy) (2.3.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from spacy) (2.32.5)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.12.3-py3-none-any.whl.metadata (87 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from spacy) (25.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.4 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.41.4-cp313-cp313-win_amd64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp313-cp313-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.2.0)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.4.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.3.1-cp313-cp313-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from jinja2->spacy) (3.0.3)\n",
      "Downloading spacy-3.8.7-cp313-cp313-win_amd64.whl (13.9 MB)\n",
      "   ---------------------------------------- 0.0/13.9 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 8.4/13.9 MB 45.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.9/13.9 MB 36.8 MB/s  0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp313-cp313-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.13-cp313-cp313-win_amd64.whl (24 kB)\n",
      "Downloading preshed-3.0.10-cp313-cp313-win_amd64.whl (115 kB)\n",
      "Downloading pydantic-2.12.3-py3-none-any.whl (462 kB)\n",
      "Downloading pydantic_core-2.41.4-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 29.6 MB/s  0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp313-cp313-win_amd64.whl (630 kB)\n",
      "   ---------------------------------------- 0.0/630.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 630.6/630.6 kB 18.0 MB/s  0:00:00\n",
      "Downloading thinc-8.3.6-cp313-cp313-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 32.2 MB/s  0:00:00\n",
      "Downloading blis-1.3.0-cp313-cp313-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.3/6.3 MB 36.5 MB/s  0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading typer-0.20.0-py3-none-any.whl (47 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "Downloading smart_open-7.4.1-py3-none-any.whl (63 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.4/5.4 MB 37.3 MB/s  0:00:00\n",
      "Downloading marisa_trie-1.3.1-cp313-cp313-win_amd64.whl (139 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: cymem, wasabi, typing-inspection, spacy-loggers, spacy-legacy, smart-open, shellingham, pydantic-core, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, annotated-types, srsly, pydantic, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "\n",
      "   - --------------------------------------  1/24 [wasabi]\n",
      "   - --------------------------------------  1/24 [wasabi]\n",
      "   ----- ----------------------------------  3/24 [spacy-loggers]\n",
      "   ----- ----------------------------------  3/24 [spacy-loggers]\n",
      "   ------ ---------------------------------  4/24 [spacy-legacy]\n",
      "   ------ ---------------------------------  4/24 [spacy-legacy]\n",
      "   ------ ---------------------------------  4/24 [spacy-legacy]\n",
      "   -------- -------------------------------  5/24 [smart-open]\n",
      "   -------- -------------------------------  5/24 [smart-open]\n",
      "   -------- -------------------------------  5/24 [smart-open]\n",
      "   ---------- -----------------------------  6/24 [shellingham]\n",
      "   ----------- ----------------------------  7/24 [pydantic-core]\n",
      "   ------------- --------------------------  8/24 [murmurhash]\n",
      "   ---------------- ----------------------- 10/24 [cloudpathlib]\n",
      "   ---------------- ----------------------- 10/24 [cloudpathlib]\n",
      "   ---------------- ----------------------- 10/24 [cloudpathlib]\n",
      "   ---------------- ----------------------- 10/24 [cloudpathlib]\n",
      "   ---------------- ----------------------- 10/24 [cloudpathlib]\n",
      "   ------------------ --------------------- 11/24 [catalogue]\n",
      "   -------------------- ------------------- 12/24 [blis]\n",
      "   -------------------- ------------------- 12/24 [blis]\n",
      "   --------------------- ------------------ 13/24 [annotated-types]\n",
      "   ----------------------- ---------------- 14/24 [srsly]\n",
      "   ----------------------- ---------------- 14/24 [srsly]\n",
      "   ----------------------- ---------------- 14/24 [srsly]\n",
      "   ----------------------- ---------------- 14/24 [srsly]\n",
      "   ----------------------- ---------------- 14/24 [srsly]\n",
      "   ----------------------- ---------------- 14/24 [srsly]\n",
      "   ----------------------- ---------------- 14/24 [srsly]\n",
      "   ----------------------- ---------------- 14/24 [srsly]\n",
      "   ----------------------- ---------------- 14/24 [srsly]\n",
      "   ----------------------- ---------------- 14/24 [srsly]\n",
      "   ----------------------- ---------------- 14/24 [srsly]\n",
      "   ----------------------- ---------------- 14/24 [srsly]\n",
      "   ----------------------- ---------------- 14/24 [srsly]\n",
      "   ----------------------- ---------------- 14/24 [srsly]\n",
      "   ----------------------- ---------------- 14/24 [srsly]\n",
      "   ----------------------- ---------------- 14/24 [srsly]\n",
      "   ----------------------- ---------------- 14/24 [srsly]\n",
      "   ----------------------- ---------------- 14/24 [srsly]\n",
      "   ----------------------- ---------------- 14/24 [srsly]\n",
      "   ----------------------- ---------------- 14/24 [srsly]\n",
      "   ----------------------- ---------------- 14/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [pydantic]\n",
      "   ------------------------- -------------- 15/24 [pydantic]\n",
      "   ------------------------- -------------- 15/24 [pydantic]\n",
      "   ------------------------- -------------- 15/24 [pydantic]\n",
      "   ------------------------- -------------- 15/24 [pydantic]\n",
      "   ------------------------- -------------- 15/24 [pydantic]\n",
      "   ------------------------- -------------- 15/24 [pydantic]\n",
      "   ------------------------- -------------- 15/24 [pydantic]\n",
      "   ------------------------- -------------- 15/24 [pydantic]\n",
      "   ------------------------- -------------- 15/24 [pydantic]\n",
      "   ------------------------- -------------- 15/24 [pydantic]\n",
      "   ------------------------- -------------- 15/24 [pydantic]\n",
      "   ------------------------- -------------- 15/24 [pydantic]\n",
      "   ------------------------- -------------- 15/24 [pydantic]\n",
      "   ------------------------- -------------- 15/24 [pydantic]\n",
      "   ------------------------- -------------- 15/24 [pydantic]\n",
      "   ------------------------- -------------- 15/24 [pydantic]\n",
      "   ------------------------- -------------- 15/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [preshed]\n",
      "   ---------------------------- ----------- 17/24 [language-data]\n",
      "   ---------------------------- ----------- 17/24 [language-data]\n",
      "   ---------------------------- ----------- 17/24 [language-data]\n",
      "   ---------------------------- ----------- 17/24 [language-data]\n",
      "   ---------------------------- ----------- 17/24 [language-data]\n",
      "   ---------------------------- ----------- 17/24 [language-data]\n",
      "   ---------------------------- ----------- 17/24 [language-data]\n",
      "   ---------------------------- ----------- 17/24 [language-data]\n",
      "   ---------------------------- ----------- 17/24 [language-data]\n",
      "   ---------------------------- ----------- 17/24 [language-data]\n",
      "   ------------------------------ --------- 18/24 [typer]\n",
      "   ------------------------------ --------- 18/24 [typer]\n",
      "   ------------------------------ --------- 18/24 [typer]\n",
      "   ------------------------------ --------- 18/24 [typer]\n",
      "   ------------------------------- -------- 19/24 [langcodes]\n",
      "   ------------------------------- -------- 19/24 [langcodes]\n",
      "   --------------------------------- ------ 20/24 [confection]\n",
      "   ----------------------------------- ---- 21/24 [weasel]\n",
      "   ----------------------------------- ---- 21/24 [weasel]\n",
      "   ----------------------------------- ---- 21/24 [weasel]\n",
      "   ----------------------------------- ---- 21/24 [weasel]\n",
      "   ----------------------------------- ---- 21/24 [weasel]\n",
      "   ----------------------------------- ---- 21/24 [weasel]\n",
      "   ----------------------------------- ---- 21/24 [weasel]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   ---------------------------------------- 24/24 [spacy]\n",
      "\n",
      "Successfully installed annotated-types-0.7.0 blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.3.1 murmurhash-1.0.13 preshed-3.0.10 pydantic-2.12.3 pydantic-core-2.41.4 shellingham-1.5.4 smart-open-7.4.1 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 typer-0.20.0 typing-inspection-0.4.2 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafb8249",
   "metadata": {},
   "source": [
    "## ðŸš€ **Step 3: Advanced NLP with spaCy**\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ **Why spaCy?**\n",
    "**spaCy** is an industrial-strength NLP library that offers:\n",
    "- **âš¡ Lightning-fast performance** (written in Cython)\n",
    "- **ðŸ§  Pre-trained neural networks** for accurate analysis\n",
    "- **ðŸ” Named Entity Recognition (NER)** out of the box\n",
    "- **ðŸŒ Multi-language support** with specialized models\n",
    "- **ðŸ“Š Dependency parsing** for understanding sentence structure\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ› ï¸ **spaCy vs NLTK Comparison**\n",
    "\n",
    "| **Feature** | **NLTK** | **spaCy** | **Best For** |\n",
    "|-------------|----------|-----------|--------------|\n",
    "| ðŸŽ“ **Learning Curve** | Steeper | Gentle | spaCy wins |\n",
    "| âš¡ **Speed** | Slower | **Much Faster** | spaCy wins |\n",
    "| ðŸ” **NER** | Basic | **Advanced** | spaCy wins |\n",
    "| ðŸ“š **Research** | Extensive | Focused | NLTK wins |\n",
    "| ðŸ­ **Production** | Good | **Excellent** | spaCy wins |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“¦ **Required Downloads**\n",
    "1. **spaCy Library**: Core NLP framework\n",
    "2. **Language Model**: `en_core_web_sm` (English small model ~15MB)\n",
    "\n",
    "**Alternative Models**:\n",
    "- `en_core_web_md` - Medium model (~50MB) - Better accuracy\n",
    "- `en_core_web_lg` - Large model (~750MB) - Best accuracy\n",
    "- `en_core_web_trf` - Transformer model (~560MB) - State-of-the-art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25d83405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 42.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 37.3 MB/s  0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Download the English language model for spaCy\n",
    "# This command is CORRECT and recommended!\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d279fd",
   "metadata": {},
   "source": [
    "### âœ… **Command Verification**\n",
    "\n",
    "**Your command is absolutely correct!** \n",
    "\n",
    "```bash\n",
    "!python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ” **Why This Command Works**\n",
    "| **Component** | **Purpose** | **Explanation** |\n",
    "|---------------|-------------|-----------------|\n",
    "| `!` | Jupyter magic | Execute shell command in notebook |\n",
    "| `python -m` | Module execution | Run spaCy as a Python module |\n",
    "| `spacy download` | spaCy CLI command | Built-in download functionality |\n",
    "| `en_core_web_sm` | Model identifier | English small model (15MB) |\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ›¡ï¸ **Alternative Download Methods**\n",
    "```bash\n",
    "# Method 1: Direct pip install (also works)\n",
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl\n",
    "\n",
    "# Method 2: Your method (RECOMMENDED)\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Method 3: Command line (outside notebook)\n",
    "# spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "> **ðŸ’¡ Pro Tip**: The `python -m spacy download` method is preferred because it ensures compatibility with your current Python environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ac2735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test spaCy installation and model loading\n",
    "import spacy\n",
    "\n",
    "print(\"ðŸ” Testing spaCy Installation...\")\n",
    "try:\n",
    "    # Load the English model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"âœ… spaCy English model loaded successfully!\")\n",
    "    \n",
    "    # Test with our IT support query\n",
    "    doc = nlp(\"My laptop is overheating after the latest update.\")\n",
    "    \n",
    "    print(\"\\nðŸŽ¯ spaCy Analysis Results:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(\"\\nðŸ·ï¸ Token Analysis:\")\n",
    "    for token in doc:\n",
    "        print(f\"{token.text:12} | {token.pos_:8} | {token.lemma_:12} | {token.is_stop}\")\n",
    "    \n",
    "    print(\"\\nðŸ” Named Entities:\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\"{ent.text:15} â†’ {ent.label_:10} ({spacy.explain(ent.label_)})\")\n",
    "    \n",
    "    if not doc.ents:\n",
    "        print(\"   No named entities detected in this query\")\n",
    "        \n",
    "except OSError as e:\n",
    "    print(f\"âŒ Model not found: {e}\")\n",
    "    print(\"ðŸ’¡ Run the download command above first!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac573d72",
   "metadata": {},
   "source": [
    "## âœ… **Command Confirmation: You Are Absolutely Correct!**\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ **Your Command Analysis**\n",
    "```bash\n",
    "!python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "**âœ… PERFECT!** This is the **officially recommended** method for downloading spaCy models.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ” **Why This Command is Ideal**\n",
    "\n",
    "| **Advantage** | **Explanation** | **Benefit** |\n",
    "|---------------|-----------------|-------------|\n",
    "| ðŸ **Environment Aware** | Uses your current Python interpreter | No version conflicts |\n",
    "| ðŸ“¦ **Module Execution** | Runs spaCy's built-in CLI | Official method |\n",
    "| ðŸ›¡ï¸ **Safe Installation** | Handles dependencies automatically | Reliable download |\n",
    "| ðŸŽ¯ **Notebook Compatible** | Works perfectly in Jupyter | No need to switch terminals |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸš€ **What Happens When You Run It**\n",
    "1. **ðŸ“¥ Downloads** `en_core_web_sm` model (~15MB)\n",
    "2. **ðŸ“ Installs** to your Python environment\n",
    "3. **ðŸ”— Links** model for easy loading with `spacy.load()`\n",
    "4. **âœ… Verifies** installation integrity\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Š **Expected Output**\n",
    "```\n",
    "Collecting en-core-web-sm==3.7.1\n",
    "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl\n",
    "Installing collected packages: en-core-web-sm\n",
    "Successfully installed en-core-web-sm-3.7.1\n",
    "âœ” Download and installation successful\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "> **ðŸ† Conclusion**: Your command is not just correctâ€”it's the **best practice** for installing spaCy models in Jupyter notebooks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55cc3f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities, their Labels, and Descriptions:\n",
      "Apple: ORG (Companies, agencies, institutions, etc.)\n",
      "San Francisco: GPE (Countries, cities, states)\n",
      "$1 billion: MONEY (Monetary values, including unit)\n",
      "Tim Cook: PERSON (People, including fictional)\n",
      "New York: GPE (Countries, cities, states)\n",
      "April 25, 2023: DATE (Absolute or relative dates or periods)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Apple is looking at buying a startup in San Francisco for $1 billion. Tim Cook attended a meeting in New York on April 25, 2023.\"\n",
    "\n",
    "# Process text through the model\n",
    "doc = nlp(text)\n",
    "\n",
    "# Perform Named Entity Recognition\n",
    "print(\"Named Entities, their Labels, and Descriptions:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text}: {ent.label_} ({spacy.explain(ent.label_)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c6b0fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: torch in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from transformers) (3.20.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from transformers) (2025.10.23)\n",
      "Requirement already satisfied: requests in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from requests->transformers) (2025.10.5)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 7.1/12.0 MB 37.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 33.9 MB/s  0:00:00\n",
      "Downloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "   ---------------------------------------- 0.0/564.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 564.3/564.3 kB 19.0 MB/s  0:00:00\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 33.7 MB/s  0:00:00\n",
      "Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "\n",
      "   ---------------------------------------- 0/4 [safetensors]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   ---------- ----------------------------- 1/4 [huggingface-hub]\n",
      "   -------------------- ------------------- 2/4 [tokenizers]\n",
      "   -------------------- ------------------- 2/4 [tokenizers]\n",
      "   -------------------- ------------------- 2/4 [tokenizers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [transformers]\n",
      "   ---------------------------------------- 4/4 [transformers]\n",
      "\n",
      "Successfully installed huggingface-hub-0.35.3 safetensors-0.6.2 tokenizers-0.22.1 transformers-4.57.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f9c6862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\transformers\\activations_tf.py:22\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Initialize sentiment analysis pipeline\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m sentiment_analyzer = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentiment-analysis\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Sample text for sentiment analysis\u001b[39;00m\n\u001b[32m      7\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m'\u001b[39m\u001b[33mm so happy with the excellent service and support I received!\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:1027\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     model_classes = {\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m-> \u001b[39m\u001b[32m1027\u001b[39m     framework, model = \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m hub_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = model.config._commit_hash\n\u001b[32m   1039\u001b[39m \u001b[38;5;66;03m# Check which preprocessing classes the pipeline uses\u001b[39;00m\n\u001b[32m   1040\u001b[39m \u001b[38;5;66;03m# None values indicate optional classes that the pipeline can run without, we don't raise errors if loading fails\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\transformers\\pipelines\\base.py:268\u001b[39m, in \u001b[36minfer_framework_load_model\u001b[39m\u001b[34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[39m\n\u001b[32m    266\u001b[39m         classes.append(_class)\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m look_tf:\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     _class = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTF\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43marchitecture\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    269\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    270\u001b[39m         classes.append(_class)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2347\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python313\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     86\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1026\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py:26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_tf_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     28\u001b[39m     TFBaseModelOutput,\n\u001b[32m     29\u001b[39m     TFMaskedLMOutput,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     TFTokenClassifierOutput,\n\u001b[32m     34\u001b[39m )\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_tf_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     36\u001b[39m     TFMaskedLanguageModelingLoss,\n\u001b[32m     37\u001b[39m     TFModelInputType,\n\u001b[32m   (...)\u001b[39m\u001b[32m     46\u001b[39m     unpack_inputs,\n\u001b[32m     47\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\transformers\\activations_tf.py:27\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras.__version__).major > \u001b[32m2\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     28\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`pip install tf-keras`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m         )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_gelu\u001b[39m(x):\n\u001b[32m     35\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[33;03m    Gaussian Error Linear Unit. Original Implementation of the gelu activation function in Google Bert repo when\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03m    initially created. For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) Also see\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33;03m    https://huggingface.co/papers/1606.08415\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize sentiment analysis pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Sample text for sentiment analysis\n",
    "text = \"I'm so happy with the excellent service and support I received!\"\n",
    "\n",
    "# Analyze sentiment\n",
    "result = sentiment_analyzer(text)\n",
    "\n",
    "# Display result\n",
    "print(\"Sentiment Analysis Result:\")\n",
    "for res in result:\n",
    "    print(f\"Label: {res['label']}, Confidence: {res['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed7e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the transformers compatibility issue\n",
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4949cc7d",
   "metadata": {},
   "source": [
    "## âš ï¸ **Sentiment Analysis Troubleshooting Guide**\n",
    "\n",
    "---\n",
    "\n",
    "### âŒ **Root Cause of the Error**\n",
    "The sentiment analysis failed due to a **Keras version compatibility issue**:\n",
    "\n",
    "```\n",
    "ValueError: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”§ **Three Solutions to Fix This**\n",
    "\n",
    "#### **Solution 1: Install tf-keras (Recommended)**\n",
    "```python\n",
    "!pip install tf-keras\n",
    "```\n",
    "- **âœ… Pros**: Maintains TensorFlow compatibility\n",
    "- **âš ï¸ Cons**: Adds another dependency\n",
    "\n",
    "#### **Solution 2: Use PyTorch-only Pipeline**\n",
    "```python\n",
    "# Force PyTorch backend (no TensorFlow)\n",
    "from transformers import pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", framework=\"pt\")\n",
    "```\n",
    "- **âœ… Pros**: Avoids TensorFlow issues entirely\n",
    "- **âš ï¸ Cons**: Requires PyTorch models\n",
    "\n",
    "#### **Solution 3: Alternative Libraries**\n",
    "```python\n",
    "# Use TextBlob for simple sentiment analysis\n",
    "from textblob import TextBlob\n",
    "text = \"I'm so happy!\"\n",
    "blob = TextBlob(text)\n",
    "print(f\"Sentiment: {blob.sentiment}\")\n",
    "```\n",
    "- **âœ… Pros**: Lightweight, no model downloads\n",
    "- **âš ï¸ Cons**: Less accurate than transformer models\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ **Why This Happens**\n",
    "- **Transformers library** defaults to TensorFlow models\n",
    "- **Keras 3** changed internal structure \n",
    "- **tf-keras** provides backward compatibility\n",
    "- **Industry transition** period between Keras versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b36a3013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\transformers\\activations_tf.py:22\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Initialize sentiment analysis pipeline\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m sentiment_analyzer = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msentiment-analysis\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Analyze the sentiment of the query\u001b[39;00m\n\u001b[32m      7\u001b[39m result = sentiment_analyzer(query)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:1027\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     model_classes = {\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m-> \u001b[39m\u001b[32m1027\u001b[39m     framework, model = \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m hub_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = model.config._commit_hash\n\u001b[32m   1039\u001b[39m \u001b[38;5;66;03m# Check which preprocessing classes the pipeline uses\u001b[39;00m\n\u001b[32m   1040\u001b[39m \u001b[38;5;66;03m# None values indicate optional classes that the pipeline can run without, we don't raise errors if loading fails\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\transformers\\pipelines\\base.py:268\u001b[39m, in \u001b[36minfer_framework_load_model\u001b[39m\u001b[34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[39m\n\u001b[32m    266\u001b[39m         classes.append(_class)\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m look_tf:\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     _class = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTF\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43marchitecture\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    269\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    270\u001b[39m         classes.append(_class)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2347\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python313\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     86\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1026\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py:26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_tf_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     28\u001b[39m     TFBaseModelOutput,\n\u001b[32m     29\u001b[39m     TFMaskedLMOutput,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     TFTokenClassifierOutput,\n\u001b[32m     34\u001b[39m )\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_tf_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     36\u001b[39m     TFMaskedLanguageModelingLoss,\n\u001b[32m     37\u001b[39m     TFModelInputType,\n\u001b[32m   (...)\u001b[39m\u001b[32m     46\u001b[39m     unpack_inputs,\n\u001b[32m     47\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\transformers\\activations_tf.py:27\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras.__version__).major > \u001b[32m2\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     28\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`pip install tf-keras`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m         )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_gelu\u001b[39m(x):\n\u001b[32m     35\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[33;03m    Gaussian Error Linear Unit. Original Implementation of the gelu activation function in Google Bert repo when\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03m    initially created. For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) Also see\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33;03m    https://huggingface.co/papers/1606.08415\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize sentiment analysis pipeline\n",
    "sentiment_analyzer = pipeline('sentiment-analysis')\n",
    "\n",
    "# Analyze the sentiment of the query\n",
    "result = sentiment_analyzer(query)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
