{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45235bd3",
   "metadata": {},
   "source": [
    "# Preparing a dataset for fine-tuning\n",
    "## Introduction\n",
    "Have you ever wondered how to turn raw data into something your machine learning model can use? In this guide, we’ll cover how to take unstructured text data, clean it up, and prepare it for fine-tuning, turning it into a powerful asset for your AI projects.\n",
    "\n",
    "By the end of this reading, you’ll be able to:\n",
    "- Efficiently pre-process your own data, ensuring it's ready for training and optimized for success in real-world tasks.\n",
    "- Prepare a dataset for fine-tuning.\n",
    "- Clean the raw data, tokenize the text, handle missing data, and structure it into a training-ready input for a fine-tuning task.\n",
    "\n",
    "## Instructions for preparing a dataset for fine-tuning\n",
    "Create a new Jupyter notebook. You can call it \"preparing_for_fine_tuning\". Make sure you have the appropriate Python kernel selected.\n",
    "\n",
    "The remaining of this reading will guide you through the following steps:\n",
    "- Step 1: Importing your dataset\n",
    "- Step 2: Clean the text\n",
    "- Step 3: Handle missing data\n",
    "- Step 4: Tokenization\n",
    "- Step 5: Structure data for fine-tuning\n",
    "- Step 6: Split the dataset\n",
    "\n",
    "### Step 1: Import data set\n",
    "You can download the Tweet emotion intensity dataset from Hugging Face into your environment.\n",
    "\n",
    "Import the file and print out the first few lines of it.\n",
    "\n",
    "The following code snippet will help you load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6e2685e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded and saved to data/tweet_emotion_intensity.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>sentiment_intensity</th>\n",
       "      <th>class_intensity</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40815</td>\n",
       "      <td>Loved @Bethenny independence msg on @WendyWill...</td>\n",
       "      <td>fear</td>\n",
       "      <td>low</td>\n",
       "      <td>fear_low</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10128</td>\n",
       "      <td>@mark_slifer actually maybe we were supposed t...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>high</td>\n",
       "      <td>sadness_high</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40476</td>\n",
       "      <td>I thought the nausea and headaches had passed ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>medium</td>\n",
       "      <td>fear_medium</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20813</td>\n",
       "      <td>Anger, resentment, and hatred are the destroye...</td>\n",
       "      <td>anger</td>\n",
       "      <td>high</td>\n",
       "      <td>anger_high</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40796</td>\n",
       "      <td>new tires &amp;amp; an alarm system on my car. fwm...</td>\n",
       "      <td>fear</td>\n",
       "      <td>low</td>\n",
       "      <td>fear_low</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet    class  \\\n",
       "0  40815  Loved @Bethenny independence msg on @WendyWill...     fear   \n",
       "1  10128  @mark_slifer actually maybe we were supposed t...  sadness   \n",
       "2  40476  I thought the nausea and headaches had passed ...     fear   \n",
       "3  20813  Anger, resentment, and hatred are the destroye...    anger   \n",
       "4  40796  new tires &amp; an alarm system on my car. fwm...     fear   \n",
       "\n",
       "  sentiment_intensity class_intensity  labels  \n",
       "0                 low        fear_low       4  \n",
       "1                high    sadness_high       9  \n",
       "2              medium     fear_medium       5  \n",
       "3                high      anger_high       0  \n",
       "4                 low        fear_low       4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "dataset = load_dataset(\"stepp1/tweet_emotion_intensity\", split=\"train\")\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"data/tweet_emotion_intensity.csv\", index=False)\n",
    "\n",
    "print(\"Dataset downloaded and saved to data/tweet_emotion_intensity.csv\")\n",
    "\n",
    "data = pd.read_csv(\"data/tweet_emotion_intensity.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8887d573",
   "metadata": {},
   "source": [
    "Once the dataset is loaded, you will use it throughout the rest of the guide to implement each of the steps involved in preparing a dataset for fine-tuning an LLM.\n",
    "\n",
    "## Types of data\n",
    "**Labeled data:** The dataset must include labeled data for supervised learning tasks such as sentiment analysis. In our case, the IMDB dataset is labeled with sentiment classes, such as \"positive\" or \"negative\" reviews.\n",
    "\n",
    "**Unlabeled data:** Unlabeled data can be used in unsupervised learning tasks or semi-supervised learning models. For this example, however, we will focus on labeled data for fine-tuning.\n",
    "\n",
    "## Sources of data\n",
    "**Public datasets:** Many open-source datasets are available for various natural language processing (NLP) tasks. Here are a few examples:\n",
    "- **IMDB Movie Reviews:** A large dataset for sentiment analysis, labeled as positive or negative reviews\n",
    "- **SQuAD:** A dataset for question-answering tasks\n",
    "- **AG News:** A dataset for text classification (e.g., categorizing news articles into topics)\n",
    "\n",
    "**Proprietary data:** You might need proprietary datasets if you’re working on a specific task in a specialized domain. For example, a healthcare LLM model might use electronic health records (EHRs) data, while a retail model could rely on customer feedback or transaction data.\n",
    "\n",
    "Starting with a clear and well-organized dataset sets a solid foundation for the fine-tuning process. Following this example with the IMDB dataset will help you practice the steps outlined in this guide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d114cf1c",
   "metadata": {},
   "source": [
    "### Step 2: Clean the text\n",
    "This step is cleaning the raw text data to remove unnecessary characters, such as URLs, special symbols, or HTML tags, and to normalize the text by converting it to lowercase. \n",
    "\n",
    "Make a new column called cleanedText that is equal to the data in the Tweet column that has had this cleanedText function applied to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40b2e80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    loved bethenny independence msg on wendywillia...\n",
      "1    mark_slifer actually maybe we were supposed to...\n",
      "2    i thought the nausea and headaches had passed ...\n",
      "3    anger resentment and hatred are the destroyer ...\n",
      "4      new tires amp an alarm system on my car fwm now\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re # Import the `re` module for working with regular expressions\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    text = text.lower() # Convert all text to lowercase for uniformity\n",
    "    text = re.sub(r'http\\S+', '', text) # Remove URLs from the text\n",
    "    text = re.sub(r'<.*?>', '', text) # Remove any HTML tags from the text\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation, keep only words and spaces\n",
    "    return text # Return the cleaned text\n",
    "\n",
    "# Assume `data` is a pandas DataFrame with a column named 'text'\n",
    "# Apply the cleaning function to each row of the 'text' column\n",
    "data['cleaned_text'] = data['tweet'].apply(clean_text)\n",
    "\n",
    "# Print the first 5 rows of the cleaned text to verify the cleaning process\n",
    "print(data['cleaned_text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58895a96",
   "metadata": {},
   "source": [
    "### Step 3: Handle missing data\n",
    "We now handle missing or incomplete data in your dataset. You can either remove rows with missing data or fill them with placeholders, ensuring the dataset is complete for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be64248d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                     0\n",
      "tweet                  0\n",
      "class                  0\n",
      "sentiment_intensity    0\n",
      "class_intensity        0\n",
      "labels                 0\n",
      "cleaned_text           0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BenkteshSharma\\AppData\\Local\\Temp\\ipykernel_7260\\3284956397.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['cleaned_text'].fillna('unknown', inplace=True) # Replace NaN values in 'cleaned_text' with 'unknown'\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the dataset\n",
    "print(data.isnull().sum()) # Print the count of missing values for each column\n",
    "\n",
    "# Option 1: Remove rows with missing data in the 'cleaned_text' column\n",
    "data = data.dropna(subset=['cleaned_text']) # Drop rows where 'cleaned_text' is NaN (missing)\n",
    "\n",
    "# Option 2: Fill missing values in 'cleaned_text' with a placeholder\n",
    "data['cleaned_text'].fillna('unknown', inplace=True) # Replace NaN values in 'cleaned_text' with 'unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a6c7b",
   "metadata": {},
   "source": [
    "### Step 4: Tokenization\n",
    "After cleaning the text, we tokenize it. Tokenization splits the text into individual words or subwords that can be used by the model. We will use the BERT tokenizer to ensure compatibility with the Brie-trained model you are fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a9083b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  3866,  7014,  2368,  4890,  4336,  5796,  2290,  2006, 12815,\n",
      "         29602,  6632,  5244,  2022,  3407, 23713, 16829,  2306,  4426, 23713,\n",
      "         13433, 28032,  7730,  2097, 19311,  2000,  2017,  3407,  2981,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  2928,  1035, 22889, 23780,  2941,  2672,  2057,  2020,  4011,\n",
      "          2000,  3280,  1998,  2026, 13445,  5552,  2256,  3268, 27451,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  1045,  2245,  1996, 19029,  1998, 14978,  2015,  2018,  2979,\n",
      "          2021,  8840,  2140,  1045,  2514,  9643,  2651,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  4963, 20234,  1998, 11150,  2024,  1996,  9799,  1997,  2115,\n",
      "          7280,  2651,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  2047, 13310, 23713,  2019,  8598,  2291,  2006,  2026,  2482,\n",
      "          1042,  2860,  2213,  2085,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "tokens = tokenizer(\n",
    "    data['cleaned_text'].tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(tokens['input_ids'][:5])  # Preview the first 5 tokenized examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5fab5a",
   "metadata": {},
   "source": [
    "We load the BERT tokenizer from the Transformers library from HuggingFace, and tokenize the cleanedText that we defined earlier. Then, we can print the tokens of the input IDs. The words have been converted into these numbered tokens. \n",
    "\n",
    "In certain cases, especially when data is limited, data augmentation techniques can be applied to generate new training examples by modifying the original dataset.\n",
    "\n",
    "* **Paraphrasing**: rewriting sentences in different ways while preserving the meaning\n",
    "\n",
    "* **Backtranslation**: translating text into another language and back again to create variation\n",
    "\n",
    "* **Synonym replacement**: replacing certain words in the text with their synonyms\n",
    "\n",
    "Code example for synonym replacement (augmentation)\n",
    "The following example demonstrates how to implement synonym replacement using the nltk library. It randomly replaces words in the text with their synonyms to create new variations of sentences. This method can be applied when paraphrasing or backtranslation is not feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14733cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import random # Random module for generating random numbers and selections\n",
    "from nltk.corpus import wordnet # NLTK's WordNet corpus for finding synonyms\n",
    "\n",
    "# Define a function to find and replace a word with a synonym\n",
    "def synonym_replacement(word):\n",
    "# Get all synsets (sets of synonyms) for the given word from WordNet\n",
    "    synonyms = wordnet.synsets(word)\n",
    "\n",
    "# If the word has synonyms, randomly choose one synonym, otherwise return the original word\n",
    "    if synonyms:\n",
    "# Select a random synonym and get the first lemma (word form) of that synonym\n",
    "        return random.choice(synonyms).lemmas()[0].name()\n",
    "\n",
    "# If no synonyms are found, return the original word\n",
    "    return word\n",
    "\n",
    "# Define a function to augment text by replacing words with synonyms randomly\n",
    "def augment_text(text):\n",
    "# Split the input text into individual words\n",
    "    words = text.split() # Split the input text into individual words\n",
    "\n",
    "# Replace each word with a synonym with a probability of 20% (random.random() > 0.8)\n",
    "    augmented_words = [\n",
    "    synonym_replacement(word) if random.random() > 0.8 else word \n",
    "# If random condition met, replace\n",
    "for word in words] # Iterate over each word in the original text\n",
    "\n",
    "# Join the augmented words back into a single string and return it\n",
    "    return ' '.join(augmented_words)\n",
    "\n",
    "# Apply the text augmentation function to the 'cleaned_text' column in a DataFrame\n",
    "# Create a new column 'augmented_text' containing the augmented version of 'cleaned_text'\n",
    "data['augmented_text'] = data['cleaned_text'].apply(augment_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3f502a",
   "metadata": {},
   "source": [
    "Explanation of code\n",
    "synonym_replacement: this function uses the nltk library’s wordnet to retrieve synonyms of a given word. If synonyms are available, it randomly selects one. If not, the original word is returned.\n",
    "\n",
    "augment_text: this function iterates through each word in the text, replacing it with a synonym based on a random probability (here, a 20 percent chance for each word).\n",
    "\n",
    "Applying augmentation: we apply the augment_text function to the cleaned text in the dataset, creating a new column, augmented_text, which contains the augmented text samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3333b2f1",
   "metadata": {},
   "source": [
    "###  Step 5: Structure the data for fine-tuning\n",
    "You can fine-tune your model once the dataset is cleaned and tokenized. The next step is structuring the data for fine-tuning. \n",
    "\n",
    "Import Torch, TensorDataset and DataLoader. We will convert the tokens into PyTorch tensors. We will define a mapping function that sets the tweet sentiment intensity from high to 1, from medium to 0.5, and from low to 0. Then, we will apply that function to each item in sentiment_intensity, and then we will drop any rows where sentiment_intensity is none, where sentiment_intensity was something other than high, medium, or low. Finally, we will convert the sentiment_intensity column to a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2648b144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # Import PyTorch library\n",
    "from torch.utils.data import TensorDataset, DataLoader # Import modules to create datasets and data loaders\n",
    "\n",
    "# Convert tokenized data into PyTorch tensors\n",
    "input_ids = tokens['input_ids'] # Extract input IDs from the tokenized data\n",
    "attention_masks = tokens['attention_mask'] # Extract attention masks from the tokenized data\n",
    "\n",
    "# Define a mapping function\n",
    "def map_sentiment(value):\n",
    "    if value == \"high\":\n",
    "        return 1\n",
    "    elif value == \"medium\":\n",
    "        return 0.5\n",
    "    elif value == \"low\":\n",
    "        return 0\n",
    "    else:\n",
    "        return None  # Handle unexpected values, if any\n",
    "\n",
    "# Apply the function to each item in 'sentiment_intensity'\n",
    "data['sentiment_intensity'] = data['sentiment_intensity'].apply(map_sentiment)\n",
    "\n",
    "# Drop any rows where 'sentiment_intensity' is None\n",
    "data = data.dropna(subset=['sentiment_intensity']).reset_index(drop=True)\n",
    "\n",
    "# Convert the 'sentiment_intensity' column to a tensor\n",
    "labels = torch.tensor(data['sentiment_intensity'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83254211",
   "metadata": {},
   "source": [
    "Following these steps, your dataset will be appropriately cleaned, tokenized, and structured for fine-tuning. A well-prepared dataset is crucial for achieving high performance and ensuring your model generalizes well to new data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1286f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training, validation, and test sets are prepared with attention masks!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split # Import function to split dataset\n",
    "\n",
    "# First split: 15% for test set, the rest for training/validation\n",
    "train_val_inputs, test_inputs, train_val_masks, test_masks, train_val_labels, test_labels = train_test_split(\n",
    "    input_ids, attention_masks, labels, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: 20% for validation set from remaining data\n",
    "train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "    train_val_inputs, train_val_masks, train_val_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create TensorDataset objects for each set, including attention masks\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "print(\"Training, validation, and test sets are prepared with attention masks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee555f0",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "A well-prepared dataset is the foundation of a successful fine-tuning process for large language models. By carefully collecting, cleaning, and tokenizing the data, you ensure that your model learns from high-quality inputs and generalizes well to unseen data. Additionally, using augmentation techniques when appropriate can further improve model performance. These steps will set you on the right path toward achieving optimal results in your fine-tuning efforts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
