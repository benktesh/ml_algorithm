{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccb0620e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (1.7.2)\n",
      "Collecting logging\n",
      "  Downloading logging-0.4.9.6.tar.gz (96 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [34 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m2\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "          \u001b[31mexec\u001b[0m\u001b[1;31m(compile('''\u001b[0m\n",
      "          \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "          \u001b[1;31m# This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py\u001b[0m\n",
      "          \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "          ...<32 lines>...\n",
      "          \u001b[1;31mexec(compile(setup_py_code, filename, \"exec\"))\u001b[0m\n",
      "          \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "          \u001b[1;31m''' % ('C:\\\\Users\\\\BenkteshSharma\\\\AppData\\\\Local\\\\Temp\\\\pip-install-inoivzp5\\\\logging_29b0d79a2d7a4e56a8cebdd2ab87bee8\\\\setup.py',), \"<pip-setuptools-caller>\", \"exec\"))\u001b[0m\n",
      "          \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"<pip-setuptools-caller>\"\u001b[0m, line \u001b[35m14\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "        File \u001b[35m\"c:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\setuptools\\__init__.py\"\u001b[0m, line \u001b[35m21\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "          \u001b[1;31mimport _distutils_hack.override\u001b[0m  # noqa: F401\n",
      "          \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"c:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\_distutils_hack\\override.py\"\u001b[0m, line \u001b[35m1\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "          \u001b[31m__import__('_distutils_hack').do_override\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "        File \u001b[35m\"c:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\_distutils_hack\\__init__.py\"\u001b[0m, line \u001b[35m89\u001b[0m, in \u001b[35mdo_override\u001b[0m\n",
      "          \u001b[31mensure_local_distutils\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "        File \u001b[35m\"c:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\_distutils_hack\\__init__.py\"\u001b[0m, line \u001b[35m75\u001b[0m, in \u001b[35mensure_local_distutils\u001b[0m\n",
      "          core = importlib.import_module('distutils.core')\n",
      "        File \u001b[35m\"C:\\Program Files\\Python313\\Lib\\importlib\\__init__.py\"\u001b[0m, line \u001b[35m88\u001b[0m, in \u001b[35mimport_module\u001b[0m\n",
      "          return \u001b[31m_bootstrap._gcd_import\u001b[0m\u001b[1;31m(name[level:], package, level)\u001b[0m\n",
      "                 \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"c:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\setuptools\\_distutils\\core.py\"\u001b[0m, line \u001b[35m16\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "          from .cmd import Command\n",
      "        File \u001b[35m\"c:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py\"\u001b[0m, line \u001b[35m9\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "          import logging\n",
      "        File \u001b[35m\"C:\\Users\\BenkteshSharma\\AppData\\Local\\Temp\\pip-install-inoivzp5\\logging_29b0d79a2d7a4e56a8cebdd2ab87bee8\\logging\\__init__.py\"\u001b[0m, line \u001b[35m618\u001b[0m\n",
      "          raise NotImplementedError\u001b[1;31m,\u001b[0m 'emit must be implemented '\\\n",
      "                                   \u001b[1;31m^\u001b[0m\n",
      "      \u001b[1;35mSyntaxError\u001b[0m: \u001b[35minvalid syntax\u001b[0m\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas scikit-learn logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9a5bab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            timestamp  cpu_usage  memory_usage  network_latency    disk_io  \\\n",
      "0 2024-01-01 00:00:00  54.967142     80.990332        86.496435  55.921924   \n",
      "1 2024-01-01 01:00:00  48.617357     73.869505        97.109627  66.396150   \n",
      "2 2024-01-01 02:00:00  56.476885     60.894456        84.151602  70.863945   \n",
      "3 2024-01-01 03:00:00  65.230299     50.295948        93.840769  93.876877   \n",
      "4 2024-01-01 04:00:00  47.658466     70.473350        62.127707  80.565531   \n",
      "\n",
      "   error_rate  \n",
      "0           0  \n",
      "1           0  \n",
      "2           1  \n",
      "3           0  \n",
      "4           0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   timestamp        1000 non-null   datetime64[ns]\n",
      " 1   cpu_usage        1000 non-null   float64       \n",
      " 2   memory_usage     1000 non-null   float64       \n",
      " 3   network_latency  1000 non-null   float64       \n",
      " 4   disk_io          1000 non-null   float64       \n",
      " 5   error_rate       1000 non-null   int64         \n",
      "dtypes: datetime64[ns](1), float64(4), int64(1)\n",
      "memory usage: 47.0 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic data\n",
    "n_samples = 1000\n",
    "data = {\n",
    "    'timestamp': pd.date_range(start='2024-01-01', periods=n_samples, freq='h'),\n",
    "    'cpu_usage': np.random.normal(50, 10, n_samples),       # CPU usage in percentage\n",
    "    'memory_usage': np.random.normal(60, 15, n_samples),    # Memory usage in percentage\n",
    "    'network_latency': np.random.normal(100, 20, n_samples), # Network latency in ms\n",
    "    'disk_io': np.random.normal(75, 10, n_samples),         # Disk I/O in MB/s\n",
    "    'error_rate': np.random.choice([0, 1], n_samples, p=[0.95, 0.05])  # 5% error rate\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba592dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomaly\n",
      " 1    950\n",
      "-1     50\n",
      "Name: count, dtype: int64\n",
      "            timestamp  cpu_usage  memory_usage  network_latency    disk_io  \\\n",
      "0 2024-01-01 00:00:00  54.967142     80.990332        86.496435  55.921924   \n",
      "1 2024-01-01 01:00:00  48.617357     73.869505        97.109627  66.396150   \n",
      "2 2024-01-01 02:00:00  56.476885     60.894456        84.151602  70.863945   \n",
      "3 2024-01-01 03:00:00  65.230299     50.295948        93.840769  93.876877   \n",
      "4 2024-01-01 04:00:00  47.658466     70.473350        62.127707  80.565531   \n",
      "5 2024-01-01 05:00:00  47.658630     65.902281       104.265874  61.645184   \n",
      "6 2024-01-01 06:00:00  65.792128     73.427898       100.024110  79.860363   \n",
      "7 2024-01-01 07:00:00  57.674347     69.527577        83.658227  59.526960   \n",
      "8 2024-01-01 08:00:00  45.305256     75.743291       113.184913  85.826911   \n",
      "9 2024-01-01 09:00:00  55.425600     51.971472       118.751403  70.288753   \n",
      "\n",
      "   error_rate  anomaly  \n",
      "0           0        1  \n",
      "1           0        1  \n",
      "2           1        1  \n",
      "3           0        1  \n",
      "4           0        1  \n",
      "5           0        1  \n",
      "6           0        1  \n",
      "7           0        1  \n",
      "8           0        1  \n",
      "9           0        1  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Implement anomaly detection using Isolation Forest\n",
    "def detect_anomalies(data):\n",
    "    model = IsolationForest(contamination=0.05, random_state=42)\n",
    "    model.fit(data)\n",
    "    anomalies = model.predict(data)\n",
    "    return anomalies\n",
    "\n",
    "# Detect anomalies in the dataset \n",
    "numeric_data = df.select_dtypes(include=[float, int]) # Only numeric columns \n",
    "df['anomaly'] = detect_anomalies(numeric_data)\n",
    "\n",
    "print(df['anomaly'].value_counts()) # -1 denotes an anomaly\n",
    "\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd8f4858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              timestamp  anomaly                       anomalous_columns\n",
      "37  2024-01-02 13:00:00       -1                   [error_rate, anomaly]\n",
      "38  2024-01-02 14:00:00       -1                   [error_rate, anomaly]\n",
      "62  2024-01-03 14:00:00       -1                   [error_rate, anomaly]\n",
      "132 2024-01-06 12:00:00       -1                   [error_rate, anomaly]\n",
      "179 2024-01-08 11:00:00       -1                   [error_rate, anomaly]\n",
      "192 2024-01-09 00:00:00       -1                   [error_rate, anomaly]\n",
      "208 2024-01-09 16:00:00       -1                   [error_rate, anomaly]\n",
      "241 2024-01-11 01:00:00       -1                   [error_rate, anomaly]\n",
      "245 2024-01-11 05:00:00       -1                   [error_rate, anomaly]\n",
      "251 2024-01-11 11:00:00       -1                   [error_rate, anomaly]\n",
      "262 2024-01-11 22:00:00       -1        [cpu_usage, error_rate, anomaly]\n",
      "272 2024-01-12 08:00:00       -1                   [error_rate, anomaly]\n",
      "285 2024-01-12 21:00:00       -1                   [error_rate, anomaly]\n",
      "315 2024-01-14 03:00:00       -1                   [error_rate, anomaly]\n",
      "329 2024-01-14 17:00:00       -1                   [error_rate, anomaly]\n",
      "330 2024-01-14 18:00:00       -1                   [error_rate, anomaly]\n",
      "334 2024-01-14 22:00:00       -1                   [error_rate, anomaly]\n",
      "350 2024-01-15 14:00:00       -1                   [error_rate, anomaly]\n",
      "354 2024-01-15 18:00:00       -1                   [error_rate, anomaly]\n",
      "371 2024-01-16 11:00:00       -1                   [error_rate, anomaly]\n",
      "386 2024-01-17 02:00:00       -1                   [error_rate, anomaly]\n",
      "387 2024-01-17 03:00:00       -1                   [error_rate, anomaly]\n",
      "413 2024-01-18 05:00:00       -1                   [error_rate, anomaly]\n",
      "471 2024-01-20 15:00:00       -1                               [anomaly]\n",
      "489 2024-01-21 09:00:00       -1                   [error_rate, anomaly]\n",
      "498 2024-01-21 18:00:00       -1                   [error_rate, anomaly]\n",
      "505 2024-01-22 01:00:00       -1                   [error_rate, anomaly]\n",
      "521 2024-01-22 17:00:00       -1  [network_latency, error_rate, anomaly]\n",
      "544 2024-01-23 16:00:00       -1                               [anomaly]\n",
      "586 2024-01-25 10:00:00       -1                   [error_rate, anomaly]\n",
      "602 2024-01-26 02:00:00       -1                   [error_rate, anomaly]\n",
      "626 2024-01-27 02:00:00       -1                   [error_rate, anomaly]\n",
      "639 2024-01-27 15:00:00       -1                   [error_rate, anomaly]\n",
      "671 2024-01-28 23:00:00       -1                   [error_rate, anomaly]\n",
      "675 2024-01-29 03:00:00       -1                   [error_rate, anomaly]\n",
      "676 2024-01-29 04:00:00       -1                   [error_rate, anomaly]\n",
      "716 2024-01-30 20:00:00       -1          [disk_io, error_rate, anomaly]\n",
      "720 2024-01-31 00:00:00       -1                   [error_rate, anomaly]\n",
      "735 2024-01-31 15:00:00       -1                   [error_rate, anomaly]\n",
      "739 2024-01-31 19:00:00       -1                   [error_rate, anomaly]\n",
      "758 2024-02-01 14:00:00       -1                   [error_rate, anomaly]\n",
      "779 2024-02-02 11:00:00       -1                   [error_rate, anomaly]\n",
      "826 2024-02-04 10:00:00       -1                   [error_rate, anomaly]\n",
      "844 2024-02-05 04:00:00       -1                   [error_rate, anomaly]\n",
      "861 2024-02-05 21:00:00       -1                   [error_rate, anomaly]\n",
      "872 2024-02-06 08:00:00       -1                   [error_rate, anomaly]\n",
      "903 2024-02-07 15:00:00       -1                   [error_rate, anomaly]\n",
      "910 2024-02-07 22:00:00       -1                   [error_rate, anomaly]\n",
      "933 2024-02-08 21:00:00       -1                   [error_rate, anomaly]\n",
      "940 2024-02-09 04:00:00       -1                   [error_rate, anomaly]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "# Calculate z-scores to identify anomalous values per column in anomalous rows\n",
    "z_scores = numeric_data.apply(zscore)\n",
    "\n",
    "# Function to identify anomalous columns for each row\n",
    "def find_anomalous_columns(row, threshold=3):\n",
    "    return [col for col in numeric_data.columns if abs(z_scores.loc[row.name, col]) > threshold]\n",
    "\n",
    "# Apply the function to each anomalous row\n",
    "df['anomalous_columns'] = df.apply(lambda row: find_anomalous_columns(row) if row['anomaly'] == -1 else [], axis=1)\n",
    "\n",
    "# Display rows with anomalies and their anomalous columns\n",
    "print(df[df['anomaly'] == -1][['timestamp', 'anomaly', 'anomalous_columns']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0699a6dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "DTypePromotionError",
     "evalue": "The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.ObjectDType'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDTypePromotionError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m X_train = df.drop(\u001b[33m'\u001b[39m\u001b[33manomaly\u001b[39m\u001b[33m'\u001b[39m, axis=\u001b[32m1\u001b[39m)\n\u001b[32m     12\u001b[39m y_train = df[\u001b[33m'\u001b[39m\u001b[33manomaly\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m predicted_causes = \u001b[43mroot_cause_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mroot_cause_analysis\u001b[39m\u001b[34m(X_train, y_train, X_test)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mroot_cause_analysis\u001b[39m(X_train, y_train, X_test):\n\u001b[32m      5\u001b[39m     model = DecisionTreeClassifier()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     predictions = model.predict(X_test)\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1024\u001b[39m, in \u001b[36mDecisionTreeClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input)\u001b[39m\n\u001b[32m    993\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, check_input=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    995\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[32m    996\u001b[39m \n\u001b[32m    997\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1021\u001b[39m \u001b[33;03m        Fitted estimator.\u001b[39;00m\n\u001b[32m   1022\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1030\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:252\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    248\u001b[39m check_X_params = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m    249\u001b[39m     dtype=DTYPE, accept_sparse=\u001b[33m\"\u001b[39m\u001b[33mcsc\u001b[39m\u001b[33m\"\u001b[39m, ensure_all_finite=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    250\u001b[39m )\n\u001b[32m    251\u001b[39m check_y_params = \u001b[38;5;28mdict\u001b[39m(ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_separately\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_y_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m missing_values_in_feature_mask = (\n\u001b[32m    257\u001b[39m     \u001b[38;5;28mself\u001b[39m._compute_missing_values_in_feature_mask(X)\n\u001b[32m    258\u001b[39m )\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2966\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mestimator\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_X_params:\n\u001b[32m   2965\u001b[39m     check_X_params = {**default_check_params, **check_X_params}\n\u001b[32m-> \u001b[39m\u001b[32m2966\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2967\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mestimator\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_y_params:\n\u001b[32m   2968\u001b[39m     check_y_params = {**default_check_params, **check_y_params}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:929\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m    925\u001b[39m pandas_requires_conversion = \u001b[38;5;28many\u001b[39m(\n\u001b[32m    926\u001b[39m     _pandas_dtype_needs_early_conversion(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dtypes_orig\n\u001b[32m    927\u001b[39m )\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dtype_iter, np.dtype) \u001b[38;5;28;01mfor\u001b[39;00m dtype_iter \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m     dtype_orig = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdtypes_orig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m pandas_requires_conversion \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(d == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[32m    931\u001b[39m     \u001b[38;5;66;03m# Force object if any of the dtypes is an object\u001b[39;00m\n\u001b[32m    932\u001b[39m     dtype_orig = \u001b[38;5;28mobject\u001b[39m\n",
      "\u001b[31mDTypePromotionError\u001b[39m: The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.ObjectDType'>)"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Train a decision tree for root cause analysis\n",
    "def root_cause_analysis(X_train, y_train, X_test):\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    return predictions\n",
    "\n",
    "# Example root cause analysis (assuming data is preprocessed)\n",
    "X_train = df.drop('anomaly', axis=1)\n",
    "y_train = df['anomaly']\n",
    "\n",
    "predicted_causes = root_cause_analysis(X_train, y_train, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e6aa0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended solution: Restart the network service.\n"
     ]
    }
   ],
   "source": [
    "# Example solution recommendation based on root cause\n",
    "def recommend_solution(root_cause):\n",
    "    solutions = {\n",
    "        \"network_error\": \"Restart the network service.\",\n",
    "        \"database_issue\": \"Check the database connection and restart the service.\",\n",
    "        \"high_cpu_usage\": \"Optimize running processes or allocate more resources.\"\n",
    "    }\n",
    "    return solutions.get(root_cause, \"No recommendation available.\")\n",
    "\n",
    "# Recommend a solution based on a detected root cause\n",
    "solution = recommend_solution(\"network_error\")\n",
    "print(f\"Recommended solution: {solution}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
