{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45a9fd3f",
   "metadata": {},
   "source": [
    "# Step 1: Set up the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbec641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch==1.13.1 (from versions: 2.6.0, 2.7.0, 2.7.1, 2.8.0, 2.9.0)\n",
      "ERROR: No matching distribution found for torch==1.13.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers==0.15.2\n",
      "  Using cached tokenizers-0.15.2.tar.gz (320 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [22 lines of output]\n",
      "      Checking for Rust toolchain....\n",
      "      Rust not found, installing into a temporary directory\n",
      "      Python reports SOABI: cp313-win_amd64\n",
      "      Computed rustc target triple: x86_64-pc-windows-msvc\n",
      "      Installation directory: C:\\Users\\BenkteshSharma\\AppData\\Local\\puccinialin\\puccinialin\\Cache\n",
      "      Rustup already downloaded\n",
      "      Installing rust to C:\\Users\\BenkteshSharma\\AppData\\Local\\puccinialin\\puccinialin\\Cache\\rustup\n",
      "      warn: It looks like you have an existing rustup settings file at:\n",
      "      warn: C:\\Users\\BenkteshSharma\\.rustup\\settings.toml\n",
      "      warn: Rustup will install the default toolchain as specified in the settings file,\n",
      "      warn: instead of the one inferred from the default host triple.\n",
      "      info: profile set to 'minimal'\n",
      "      info: default host triple is x86_64-pc-windows-msvc\n",
      "      warn: Updating existing toolchain, profile choice will be ignored\n",
      "      info: syncing channel updates for 'stable-x86_64-pc-windows-msvc'\n",
      "      info: default toolchain set to 'stable-x86_64-pc-windows-msvc'\n",
      "      Checking if cargo is installed\n",
      "      \n",
      "      Cargo, the Rust package manager, is not installed or is not on PATH.\n",
      "      This package requires Rust and Cargo to compile extensions. Install it through\n",
      "      the system's package manager or via https://rustup.rs/\n",
      "      \n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> tokenizers\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.36.2\n",
      "  Using cached transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "Requirement already satisfied: filelock in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from transformers==4.36.2) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from transformers==4.36.2) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from transformers==4.36.2) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from transformers==4.36.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from transformers==4.36.2) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from transformers==4.36.2) (2025.10.23)\n",
      "Requirement already satisfied: requests in c:\\repo\\scratch\\ml\\mlvenv\\lib\\site-packages (from transformers==4.36.2) (2.32.5)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.36.2)\n",
      "  Using cached tokenizers-0.15.2.tar.gz (320 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [22 lines of output]\n",
      "      Checking for Rust toolchain....\n",
      "      Rust not found, installing into a temporary directory\n",
      "      Python reports SOABI: cp313-win_amd64\n",
      "      Computed rustc target triple: x86_64-pc-windows-msvc\n",
      "      Installation directory: C:\\Users\\BenkteshSharma\\AppData\\Local\\puccinialin\\puccinialin\\Cache\n",
      "      Rustup already downloaded\n",
      "      Installing rust to C:\\Users\\BenkteshSharma\\AppData\\Local\\puccinialin\\puccinialin\\Cache\\rustup\n",
      "      warn: It looks like you have an existing rustup settings file at:\n",
      "      warn: C:\\Users\\BenkteshSharma\\.rustup\\settings.toml\n",
      "      warn: Rustup will install the default toolchain as specified in the settings file,\n",
      "      warn: instead of the one inferred from the default host triple.\n",
      "      info: profile set to 'minimal'\n",
      "      info: default host triple is x86_64-pc-windows-msvc\n",
      "      warn: Updating existing toolchain, profile choice will be ignored\n",
      "      info: syncing channel updates for 'stable-x86_64-pc-windows-msvc'\n",
      "      info: default toolchain set to 'stable-x86_64-pc-windows-msvc'\n",
      "      Checking if cargo is installed\n",
      "      \n",
      "      Cargo, the Rust package manager, is not installed or is not on PATH.\n",
      "      This package requires Rust and Cargo to compile extensions. Install it through\n",
      "      the system's package manager or via https://rustup.rs/\n",
      "      \n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> tokenizers\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1\n",
    "%pip install tokenizers==0.15.2\n",
    "%pip install transformers==4.36.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e4d23",
   "metadata": {},
   "source": [
    "# Step 2: Import libraries and load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cf353f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12cf2091ad1a4fa2b39e5ffd76847dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\BenkteshSharma\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Repo\\Scratch\\ml\\mlvenv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c5ed643062494e9af57f8cc2fae6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize sentiment analyzer with specific model to avoid downloading issues\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "print(\"Sentiment analysis model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c463f50",
   "metadata": {},
   "source": [
    "# Step 3: Input text for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample texts for sentiment analysis\n",
    "texts = [\n",
    "    \"I love this product! It's amazing.\",\n",
    "    \"The service was terrible and I'm very disappointed.\",\n",
    "    \"It's okay, not great but not bad either.\"\n",
    "]\n",
    "\n",
    "# Analyze the sentiment of each text\n",
    "for text in texts:\n",
    "    result = sentiment_analyzer(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {result[0]['label']}\")\n",
    "    print(f\"Confidence: {result[0]['score']:.2f}\")\n",
    "    print()  # Empty line for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad28a680",
   "metadata": {},
   "source": [
    "# Step 4: Test with custom input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881364b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accept user input for custom sentiment analysis\n",
    "custom_text = input(\"Enter a sentence for sentiment analysis: \")\n",
    "\n",
    "# Analyze the sentiment\n",
    "result = sentiment_analyzer(custom_text)\n",
    "\n",
    "print(f\"\\nSentiment: {result[0]['label']}\")\n",
    "print(f\"Confidence: {result[0]['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db03d53",
   "metadata": {},
   "source": [
    "# Step 5: Enhance system to do analyze long text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7ded4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow the model to process a longer paragraph of text\n",
    "long_text = \"\"\"\n",
    "The product is good overall, but there are some issues with battery life. \n",
    "I wish it lasted longer. However, the design is sleek, and I’m happy with the performance so far.\n",
    "\"\"\"\n",
    "result = sentiment_analyzer(long_text)\n",
    "for res in result:\n",
    "    print(f\"Sentiment: {res['label']}, Confidence: {res['score']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
